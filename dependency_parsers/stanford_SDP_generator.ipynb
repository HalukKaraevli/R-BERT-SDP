{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "judicial-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import stanza\n",
    "\n",
    "def return_entities(text):\n",
    "    e1 = re.match('.*\\[E11\\](.*)\\[E12\\]',text).groups()[0]\n",
    "    e2 = re.match('.*\\[E21\\](.*)\\[E22\\]',text).groups()[0]\n",
    "    return (e1,e2)\n",
    "\n",
    "\n",
    "def generate_paths_to_root(entity_tokens,\n",
    "                          term_ids_map,\n",
    "                          id_dep_id_map):\n",
    "    paths = []\n",
    "    for term in entity_tokens:\n",
    "        t_ids = term_ids_map[term]\n",
    "        for t_id in t_ids:\n",
    "            temp = t_id\n",
    "            temp_path = []\n",
    "            while temp != 0:\n",
    "                temp_path.append(temp)\n",
    "                temp = id_dep_id_map[temp]\n",
    "            paths.append(temp_path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def find_shortest_path_between_entities(e1_paths,\n",
    "                                        e2_paths):\n",
    "    paths = []\n",
    "    e1_path_sets = list(map(set,e1_paths))\n",
    "    e2_path_sets = list(map(set,e2_paths))\n",
    "    #print(e1_paths)\n",
    "    #print(e2_paths)\n",
    "    \n",
    "    for i1, p1 in enumerate(e1_paths):\n",
    "        for i2, p2 in enumerate(e2_paths):\n",
    "            common_ids = e1_path_sets[i1] & e2_path_sets[i2]\n",
    "            #print('common', common_ids)\n",
    "            temp_path = []\n",
    "            for t_id in p1:\n",
    "                temp_path.append(t_id)\n",
    "                if t_id in common_ids:\n",
    "                    break\n",
    "            #print('p1 end')\n",
    "            #print(temp_path)\n",
    "            r_p2 = list(reversed(p2))\n",
    "            for index,t_id in enumerate(r_p2):\n",
    "                if t_id not in common_ids:\n",
    "                    temp_path.extend(r_p2[index:])\n",
    "                    break\n",
    "            paths.append(temp_path)\n",
    "    \n",
    "    return min(paths,key=len)\n",
    "\n",
    "\n",
    "def calculate_sdp(sample_index,sample,exceptional_case_ids):\n",
    "    res = ['Error']\n",
    "    \n",
    "    \n",
    "    id_term_map = {int(x[0]):x[1] for x in sample}\n",
    "    term_ids_map = {}\n",
    "    for x in sample:\n",
    "        term_ids_map[x[1]] = term_ids_map.get(x[1],[])\n",
    "        term_ids_map[x[1]].append(int(x[0]))\n",
    "\n",
    "    id_dep_id_map = {int(x[0]):x[2] for x in sample}\n",
    "    temp_entities = entities_tokenized[sample_index]\n",
    "\n",
    "    if sample_index in exceptional_case_ids:\n",
    "        for i,k in enumerate(sample):\n",
    "            for ent in temp_entities:\n",
    "                for ind,e in enumerate(ent):\n",
    "                    if e.startswith(k[1]) and \\\n",
    "                    (i+1 != len(sample)) and \\\n",
    "                     e.endswith(sample[i+1][1]):\n",
    "                        ent[ind] = k[1]\n",
    "                        \n",
    "    e1 = entities_tokenized[sample_index][0]\n",
    "    #print(e1)\n",
    "    e2 = entities_tokenized[sample_index][1]\n",
    "    #print(e2)\n",
    "\n",
    "    e1_paths = generate_paths_to_root(e1,\n",
    "                                      term_ids_map,\n",
    "                                      id_dep_id_map)\n",
    "    e2_paths = generate_paths_to_root(e2,\n",
    "                                      term_ids_map,\n",
    "                                      id_dep_id_map)\n",
    "\n",
    "    shortest_path =find_shortest_path_between_entities(e1_paths,\n",
    "                                                       e2_paths)\n",
    "    res = list(id_term_map[x] for x in shortest_path)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_semeval_tsv_data_path = './R-BERT/data/test.tsv'\n",
    "stanza_path = '../stanza_en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "    lines = open(default_semeval_data_path,'r')\n",
    "    lines = lines.readlines()\n",
    "    example_list = list(map(lambda x: x.split('\\t'), lines))\n",
    "    train_sdp_text = list(map(lambda x: (x[1].replace('[E11]','')\n",
    "                                               .replace('[E12]','')\n",
    "                                               .replace('[E21]','')\n",
    "                                               .replace('[E22]','')), example_list))\n",
    "\n",
    "    entities = [return_entities(x) for x in map(lambda k: k[1], example_list)]\n",
    "    entities_tokenized = [[x.split(), y.split()] for x,y in entities]\n",
    "    entities_tokens_flat = [x.split() + y.split() for x,y in entities]\n",
    "\n",
    "\n",
    "    ### DEFAULT DEPENDENCY PARSER\n",
    "\n",
    "    nlp = stanza.Pipeline('en',stanza_path)\n",
    "\n",
    "    train_dep_parser_res = []\n",
    "    for i,text in enumerate(train_sdp_text[:3]):\n",
    "        train_dep_parser_res.append(list(term \n",
    "                                         for s in nlp(text).sentences \n",
    "                                         for term in map(lambda x: (x.id, x.text, x.head, x.deprel),\n",
    "                                                         s.words)))\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(i, len(train_sdp_text))\n",
    "\n",
    "    pickle.dump(train_dep_parser_res, open('default_test_dep_parser_results.pkl','wb'))\n",
    "    train_dep_parser_res = pickle.load(open('train_dep_parser_results.pkl','rb'))\n",
    "    dep_parser_token_sets = [set(k[1] for k in x) for x in train_dep_parser_res]\n",
    "\n",
    "    exceptional_cases = list(filter(lambda x: any(map(lambda k: k not in dep_parser_token_sets[x[0]],\n",
    "                                  x[1])), \n",
    "                              enumerate(entities_tokens_flat)))\n",
    "\n",
    "    exceptional_case_ids = set(x[0] for x in exceptional_cases)\n",
    "\n",
    "    \n",
    "    res = [calculate_sdp(index,sample,exceptional_case_ids) for index,sample in enumerate(train_dep_parser_res)]\n",
    "    \n",
    "    ### Additional Step (Item 10 from paper)\n",
    "    for i,r in enumerate(res):\n",
    "        if r[0] in entities_tokenized[i][0]:\n",
    "            r[0] = entities[i][0]\n",
    "            r[-1] = entities[i][-1]\n",
    "        else:\n",
    "            print(i)\n",
    "\n",
    "    pickle.dump(res,open('./default_train_sdp_results.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
